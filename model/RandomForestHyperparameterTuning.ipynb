{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForest HyperParameter Tuning\n",
    "\n",
    "https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaults arameters:\n",
      "{'bootstrap': True,\n",
      " 'ccp_alpha': 0.0,\n",
      " 'class_weight': None,\n",
      " 'criterion': 'gini',\n",
      " 'max_depth': None,\n",
      " 'max_features': 'auto',\n",
      " 'max_leaf_nodes': None,\n",
      " 'max_samples': None,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_impurity_split': None,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 2,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'n_estimators': 100,\n",
      " 'n_jobs': None,\n",
      " 'oob_score': False,\n",
      " 'random_state': 91,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "forest = RandomForestClassifier(random_state = 91)\n",
    "\n",
    "print('Defaults arameters:')\n",
    "pprint(forest.get_params())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters to tune**\n",
    "\n",
    "- n_estimators = number of trees in the foreset\n",
    "- max_features = max number of features considered for splitting a node\n",
    "- max_depth = max number of levels in each decision tree\n",
    "- min_samples_split = min number of data points placed in a node before the node is split\n",
    "- min_samples_leaf = min number of data points allowed in a leaf node\n",
    "- bootstrap = method for sampling data points (with or without replacement)\n",
    "\n",
    "\n",
    "# Loading our data \n",
    "\n",
    "- Selected features and model from model exercice (see **model classif selection.ipynb**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.25\n",
      "Test substations selected: \n",
      "['14th & S ST NW A', '14th & S ST NW B', 'MA_EPA']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"bigtable.csv\")\n",
    "\n",
    "def clean_dataset(df):\n",
    "    assert isinstance(df, pd.DataFrame), \"df needs to be a pd.DataFrame\"\n",
    "    df.dropna(inplace=True)\n",
    "    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)\n",
    "    return df[indices_to_keep]#.astype(np.float64).\n",
    "\n",
    "df = clean_dataset(df).reset_index()\n",
    "\n",
    "features = df[[\n",
    "    'x', \n",
    "    'y', \n",
    "    'dayofweek', \n",
    "    'sin_day', \n",
    "    'cos_day',\n",
    "    'sin_year', \n",
    "    'cos_year', \n",
    "    'TEMP', \n",
    "    'cos_wind', \n",
    "    'sin_wind', \n",
    "    'Wind-Rate', \n",
    "    'DEW', \n",
    "    'SKY', \n",
    "    'VIS', \n",
    "    'ATM'\n",
    "]].astype(np.float64)\n",
    "\n",
    "#features.loc[:,'dayofweek'] = features['dayofweek'].astype('category')\n",
    "\n",
    "gs = df[['station_id']]\n",
    "\n",
    "labels = df[[\n",
    "    'pm25',\n",
    "    'AQI_VALUE', #pm25 transformed using EPA methodology\n",
    "    'AQI_class'  #pm25 transformed into EPA categorical class\n",
    "]]\n",
    "\n",
    "labels.loc[:,\"polluted\"] = (labels.loc[:,\"AQI_class\"] != \"Good\")\n",
    "\n",
    "y=labels[\"polluted\"]\n",
    "X=features\n",
    "\n",
    "def tts_gs(X, y, gs, test_size):\n",
    "    stations = gs[\"station_id\"].unique()\n",
    "    nb_stations = len(stations) * test_size\n",
    "    print(nb_stations)\n",
    "    my_randoms = random.sample(list(stations), int(nb_stations))\n",
    "    filters = gs[\"station_id\"].isin(my_randoms)\n",
    "    print('Test substations selected: ')\n",
    "    print(my_randoms)\n",
    "    return X[~filters], X[filters], y[~filters], y[filters]\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = tts_gs(X, y, gs, 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         True\n",
       "1         True\n",
       "2        False\n",
       "3        False\n",
       "4        False\n",
       "         ...  \n",
       "97669    False\n",
       "97670    False\n",
       "97671    False\n",
       "97672    False\n",
       "97673    False\n",
       "Name: polluted, Length: 97674, dtype: bool"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = LabelEncoder().fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'estimator__bootstrap': [True, False],\n",
      " 'estimator__max_depth': [10, 22, 35, 48, 61, 74, 87, 100, None],\n",
      " 'estimator__max_features': ['auto', 'sqrt'],\n",
      " 'estimator__min_samples_leaf': [1, 2, 4],\n",
      " 'estimator__min_samples_split': [2, 3, 5],\n",
      " 'estimator__n_estimators': [100, 325, 550, 775, 1000]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 5)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 100, num = 8)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 3, 5]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]# Create the random grid\n",
    "random_grid = {'estimator__n_estimators': n_estimators,\n",
    "               'estimator__max_features': max_features,\n",
    "               'estimator__max_depth': max_depth,\n",
    "               'estimator__min_samples_split': min_samples_split,\n",
    "               'estimator__min_samples_leaf': min_samples_leaf,\n",
    "               'estimator__bootstrap': bootstrap}\n",
    "\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed: 14.0min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed: 28.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score=nan,\n",
       "                   estimator=Pipeline(memory=None,\n",
       "                                      steps=[('preprocessor',\n",
       "                                              StandardScaler(copy=True,\n",
       "                                                             with_mean=True,\n",
       "                                                             with_std=True)),\n",
       "                                             ('estimator',\n",
       "                                              RandomForestClassifier(bootstrap=True,\n",
       "                                                                     ccp_alpha=0.0,\n",
       "                                                                     class_weight=None,\n",
       "                                                                     criterion='gini',\n",
       "                                                                     max_depth=None,\n",
       "                                                                     max_features='auto',\n",
       "                                                                     max_leaf_nodes=None,\n",
       "                                                                     max_samples=None,\n",
       "                                                                     min_impurity_decrease=0.0,\n",
       "                                                                     mi...\n",
       "                   param_distributions={'estimator__bootstrap': [True, False],\n",
       "                                        'estimator__max_depth': [10, 22, 35, 48,\n",
       "                                                                 61, 74, 87,\n",
       "                                                                 100, None],\n",
       "                                        'estimator__max_features': ['auto',\n",
       "                                                                    'sqrt'],\n",
       "                                        'estimator__min_samples_leaf': [1, 2,\n",
       "                                                                        4],\n",
       "                                        'estimator__min_samples_split': [2, 3,\n",
       "                                                                         5],\n",
       "                                        'estimator__n_estimators': [100, 325,\n",
       "                                                                    550, 775,\n",
       "                                                                    1000]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=91, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=2)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "\n",
    "model = Pipeline([\n",
    "     ('preprocessor', StandardScaler()),\n",
    "     ('estimator', RandomForestClassifier(random_state = 91))\n",
    "])\n",
    "\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = model, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=91, n_jobs = -1)\n",
    "\n",
    "# Fit the random search model\n",
    "rf_random.fit(Xtrain, ytrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_features, test_labels):\n",
    "    predictions = model.predict(test_features)\n",
    "    errors = sum(predictions != test_labels)\n",
    "    trues = sum((test_labels == True))\n",
    "    falses = sum((test_labels == False))\n",
    "    true_positives = sum((predictions == test_labels) & (test_labels == True))\n",
    "    true_negatives = sum((predictions == test_labels) & (test_labels == False))\n",
    "    false_positives = sum((predictions != test_labels) & (test_labels == True))\n",
    "    false_negatives = sum((predictions != test_labels) & (test_labels == False))\n",
    "    \n",
    "    print('Model Performance')\n",
    "    print('Average Error: {:0.4f}.'.format(errors / len(test_labels)))\n",
    "    print('Average true_positives: {:0.4f}.'.format(true_positives / len(test_labels)))\n",
    "    print('Average true_negatives: {:0.4f}.'.format(true_negatives / len(test_labels)))\n",
    "    print('Average false_positives: {:0.4f}.'.format(false_positives / len(test_labels)))\n",
    "    print('Average false_negatives: {:0.4f}.'.format(errors / len(test_labels)))\n",
    "    \n",
    "    #Precision = TruePositives / (TruePositives + FalsePositives)\n",
    "    trues_precision = (true_positives / (true_positives + false_positives))\n",
    "    falses_precision = (true_negatives / (true_negatives + false_negatives))\n",
    "    print('Precision for Trues (is Polluted) = {:0.4f}%.'.format(100*trues_precision))\n",
    "    print('Precision for False (not Polluted) {:0.24}%.'.format(100*falses_precision))\n",
    "    \n",
    "    #Recall = TruePositives / (TruePositives + FalseNegatives)\n",
    "    trues_recall = true_positives / (true_positives + false_negatives)\n",
    "    falses_recall = true_negatives / (true_negatives + false_positives)\n",
    "    print('(!)Precision for Trues (is Polluted) = {:0.4f}%.'.format(100*trues_recall))\n",
    "    print('Precision for False (not Polluted) {:0.4f}%.'.format(100*falses_recall))\n",
    "    \n",
    "    #F-1 = (2 * Precision * Recall) / (Precision + Recall)\n",
    "    trues_f1 = (2 * trues_precision * trues_recall) /(trues_precision + trues_recall)\n",
    "    falses_f1 = (2 * falses_precision * falses_recall) /(falses_precision + falses_recall)\n",
    "    print('(!)F1 for Trues (is Polluted) = {:0.4f}%.'.format(100*trues_f1))\n",
    "    print('F1 for False (not Polluted) {:0.4f}%.'.format(100*falses_f1))\n",
    "    \n",
    "    # Compute and return F1 (harmonic mean of precision and recall)\n",
    "    print(\"{}: {:0.4f}%\".format(model.__class__.__name__, 100 * f1_score(test_labels, predictions)))\n",
    "    \n",
    "    return f1_score(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_model = RandomForestClassifier(n_estimators = 10, random_state = 91)\n",
    "model.fit(Xtrain, ytrain)\n",
    "base_accuracy = evaluate(model, Xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance\n",
      "Average Error: 0.1362.\n",
      "Average true_positives: 0.1338.\n",
      "Average true_negatives: 0.7300.\n",
      "Average false_positives: 0.0379.\n",
      "Average false_negatives: 0.1362.\n",
      "Precision for Trues (is Polluted) = 77.9286%.\n",
      "Precision for False (not Polluted) 88.1283533885502947669011%.\n",
      "(!)Precision for Trues (is Polluted) = 57.6315%.\n",
      "Precision for False (not Polluted) 95.0663%.\n",
      "(!)F1 for Trues (is Polluted) = 66.2606%.\n",
      "F1 for False (not Polluted) 91.4660%.\n",
      "Pipeline: 66.2606%\n"
     ]
    }
   ],
   "source": [
    "best_random = rf_random.best_estimator_\n",
    "random_accuracy = evaluate(best_random, Xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improvement of 3.15%.\n"
     ]
    }
   ],
   "source": [
    "print('Improvement of {:0.2f}%.'.format( 100 * (random_accuracy - base_accuracy) / base_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
